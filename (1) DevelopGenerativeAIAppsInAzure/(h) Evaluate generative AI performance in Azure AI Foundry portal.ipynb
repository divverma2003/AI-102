{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+uMBxghMZtVbdbhBCVxrm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Introduction**\n","---\n","Evaluating your generative AI apps is crucial for several reasons. First and foremost, it ensures quality assurance. By assessing your app's performance, you can identify and address any issues, ensuring that it provides accurate and relevant responses. High quality responses lead to improved user satisfaction. When users receive accurate and helpful responses, they're more likely to have a positive experience and continue using your application.\n","\n","Evaluation is also essential for continuous improvement. By analyzing the results of your evaluations, you can identify areas for enhancement and iteratively improve your app's performance. The ongoing process of evaluation and improvement helps you stay ahead of user needs and expectations, ensuring that your app remains effective and valuable."],"metadata":{"id":"cz2WKx0Vo_Zv"}},{"cell_type":"markdown","source":["# **Assess the model performance**\n","---\n","## Evaluation Importance\n","Crucial at different phases to ensure effectiveness and reliability of generative AI applications\n","\n","## Two Evaluation Scopes\n","\n","### 1. Individual Language Model\n","```\n","INPUT (1)\n","    ↓\n","LANGUAGE MODEL (2)\n","    ↓\n","OUTPUT (3)\n","    ↓\n","EVALUATION\n","```\n","\n","**Evaluation Process**:\n","- Analyze input\n","- Analyze output\n","- Optionally compare to predefined expected output\n","\n","**Purpose**: Decide which model to integrate into application\n","\n","### 2. Complete Chat Flow\n","```\n","INPUT (1)\n","    ↓\n","CHAT FLOW (2)\n","├─ Language Model(s)\n","└─ Python code\n","    ↓\n","OUTPUT (3)\n","    ↓\n","EVALUATION\n","```\n","\n","**Chat Flow Characteristics**:\n","- Orchestrate executable flows\n","- Combine multiple language models\n","- Include Python code\n","- Process through various nodes\n","\n","**Evaluation Scope**: Complete flow and individual components\n","\n","## Evaluation Approaches\n","\n","### Progression Strategy\n","1. Start with individual model testing\n","2. Progress to complete chat flow testing\n","3. Validate generative AI app works as expected\n","\n","## Model Benchmarks\n","\n","### Definition\n","Publicly available metrics across models and datasets\n","\n","### Purpose\n","Understand model performance relative to others\n","\n","### Common Benchmarks\n","\n","**Accuracy**:\n","- Compares model-generated text with correct answer\n","- Result: 1 (exact match) or 0 (no match)\n","- Binary assessment\n","\n","**Coherence**:\n","- Measures smooth flow of output\n","- Reads naturally\n","- Resembles human-like language\n","\n","**Fluency**:\n","- Grammatical rule adherence\n","- Syntactic structure correctness\n","- Appropriate vocabulary usage\n","- Linguistically correct responses\n","\n","**GPT Similarity**:\n","- Quantifies semantic similarity\n","- Compares ground truth with prediction\n","- Document or sentence level\n","\n","### Azure AI Foundry Access\n","**Location**: Model catalog → Model benchmarks\n","**Usage**: Explore benchmarks before deploying model\n","**Benefit**: Compare models before selection\n","\n","## Manual Evaluations\n","\n","### Method\n","Human raters assess response quality\n","\n","### Advantages\n","- Captures insights automated metrics miss\n","- Evaluates context relevance\n","- Assesses user satisfaction\n","\n","### Rating Criteria\n","- Relevance\n","- Informativeness\n","- Engagement\n","\n","### Value\n","Provides human perspective on quality aspects\n","\n","## AI-Assisted Metrics\n","\n","### Generation Quality Metrics\n","**Evaluate**:\n","- Overall text quality\n","- Creativity\n","- Coherence\n","- Adherence to desired style or tone\n","\n","**Purpose**: Comprehensive quality assessment beyond simple matching\n","\n","### Risk and Safety Metrics\n","**Assess**:\n","- Potential risks in outputs\n","- Safety concerns\n","- Harmful content generation\n","- Biased content generation\n","\n","**Purpose**: Ensure model doesn't produce harmful outputs\n","\n","## Natural Language Processing (NLP) Metrics\n","\n","### F1-Score\n","**Measures**: Ratio of shared words between generated and ground truth answers\n","\n","**Use Cases**:\n","- Text classification\n","- Information retrieval\n","- Precision and recall importance\n","\n","**Purpose**: Quantify word-level overlap\n","\n","### Common NLP Metrics\n","\n","**BLEU** (Bilingual Evaluation Understudy):\n","- Machine translation evaluation\n","- Measures precision of n-grams\n","\n","**METEOR** (Metric for Evaluation of Translation with Explicit Ordering):\n","- Translation quality assessment\n","- Considers synonyms and stemming\n","\n","**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation):\n","- Summarization evaluation\n","- Measures recall of n-grams\n","\n","### Common Purpose\n","Quantify overlap level between:\n","- Model-generated response\n","- Ground truth (expected response)\n","\n","## Evaluation Method Comparison\n","\n","| Method | Type | Best For | Key Benefit |\n","|--------|------|----------|-------------|\n","| **Model Benchmarks** | Automated | Pre-deployment comparison | Relative performance understanding |\n","| **Manual Evaluations** | Human | Context and satisfaction | Captures nuanced quality aspects |\n","| **AI-Assisted** | Automated | Quality and safety | Comprehensive assessment |\n","| **NLP Metrics** | Automated | Text overlap | Quantitative comparison |\n","\n","## Evaluation Strategy\n","\n","### Phase-Based Approach\n","\n","**Early Phase**:\n","- Model benchmarks for selection\n","- Compare available models\n","- Understand relative performance\n","\n","**Development Phase**:\n","- NLP metrics for ground truth comparison\n","- AI-assisted metrics for quality\n","- Manual evaluation samples\n","\n","**Pre-Production Phase**:\n","- Complete chat flow evaluation\n","- Risk and safety metrics\n","- Manual evaluations for user satisfaction\n","\n","**Production Phase**:\n","- Ongoing monitoring\n","- User feedback (manual evaluation)\n","- Continuous metric tracking\n","\n","## Key Considerations\n","\n","### Choosing Evaluation Methods\n","- **Automated metrics**: Fast, scalable, objective\n","- **Manual evaluations**: Insightful, context-aware, costly\n","- **Combined approach**: Most comprehensive\n","\n","### Ground Truth Requirements\n","Many metrics require predefined expected outputs:\n","- F1-score needs ground truth answers\n","- BLEU/METEOR/ROUGE need reference texts\n","- GPT similarity needs ground truth sentences\n","\n","### Evaluation Timing\n","- **Before deployment**: Model benchmarks\n","- **During development**: All metrics applicable\n","- **After deployment**: User satisfaction, safety metrics\n","\n","## Best Practices\n","\n","### Multi-Metric Approach\n","- Use multiple evaluation methods\n","- No single metric captures all aspects\n","- Balance automated and manual evaluation\n","\n","### Baseline Establishment\n","- Create evaluation baseline\n","- Track improvements over time\n","- Compare different model versions\n","\n","### Context-Specific Evaluation\n","- Tailor metrics to use case\n","- Some metrics more relevant than others\n","- Consider business requirements\n","\n","## Key Takeaway\n","Evaluate generative AI at two scopes: individual models (input→model→output) and complete chat flows (input→flow nodes→output). Use four evaluation approaches: model benchmarks for pre-deployment comparison, manual evaluations for human perspective, AI-assisted metrics for quality and safety, and NLP metrics (F1, BLEU, METEOR, ROUGE) for quantifying overlap with ground truth. Access model benchmarks in Azure AI Foundry portal before deployment."],"metadata":{"id":"2xAEmqBnDiQn"}},{"cell_type":"markdown","source":["# **Manually evaluate the performance of a model**\n","---\n","# AI-102 Study Notes: Manual Model Evaluation\n","\n","## Manual Evaluation Purpose\n","\n","### Early Development Phase\n","- Experiment and iterate quickly\n","- Assess if model meets requirements\n","- Test prompt flow applications\n","- Identify improvements needed\n","\n","### Production Phase\n","- Ongoing performance assessment\n","- Capture insights automated metrics miss\n","- Human perspective on quality\n","- Validate model behavior\n","\n","## Test Prompt Preparation\n","\n","### Requirements\n","Create **diverse set** of test prompts reflecting app's expected use\n","\n","### Coverage Areas\n","- **Common user questions**: Typical queries\n","- **Edge cases**: Unusual or boundary scenarios\n","- **Potential failure points**: Known challenging areas\n","- **Various scenarios**: Wide range of use cases\n","\n","### Purpose\n","- Comprehensive performance assessment\n","- Identify improvement areas\n","- Validate across different contexts\n","\n","## Chat Playground Testing\n","\n","### Purpose\n","Test individual deployed language model before app integration\n","\n","### Location\n","Azure AI Foundry portal → Chat playground\n","\n","### Testing Process\n","```\n","1. Enter prompt\n","    ↓\n","2. View model response\n","    ↓\n","3. Tweak prompt or system message\n","    ↓\n","4. Apply changes\n","    ↓\n","5. Test again\n","    ↓\n","6. Evaluate improvement\n","```\n","\n","### Key Features\n","- **Interactive testing**: Real-time response viewing\n","- **Iterative refinement**: Quick tweaks and retests\n","- **Parameter adjustment**: Modify settings on the fly\n","\n","### Ideal For\n","- Early development phase\n","- Quick experimentation\n","- Single prompt testing\n","- Immediate feedback\n","\n","### Configuration Parameters\n","\n","**System Message**:\n","- Defines AI behavior and personality\n","- Sets context and constraints\n","- Example: \"You are a helpful travel assistant\"\n","\n","**Temperature**:\n","- Controls response randomness/creativity\n","- Range: 0 (deterministic) to 1 (creative)\n","- Low: Focused, predictable responses\n","- High: Creative, varied responses\n","\n","**Max Response**:\n","- Maximum tokens in generated response\n","- Controls response length\n","- Prevents excessively long outputs\n","\n","## Manual Evaluations Feature\n","\n","### Purpose\n","Evaluate multiple prompts more quickly than chat playground\n","\n","### Process\n","\n","**1. Dataset Upload**:\n","- Upload dataset with multiple questions\n","- Optionally include expected responses\n","- Test on larger dataset\n","\n","**2. Response Rating**:\n","- Thumbs up/down feature\n","- Rate each model response\n","- Track overall performance\n","\n","**3. Improvement Iteration**:\n","Based on ratings, adjust:\n","- Input prompts\n","- System message\n","- Model selection\n","- Model parameters (temperature, max response)\n","\n","### Advantages Over Chat Playground\n","- **Faster evaluation**: Test multiple prompts efficiently\n","- **Systematic approach**: Consistent evaluation process\n","- **Dataset-based**: Comprehensive coverage\n","- **Comparative analysis**: Track improvements\n","\n","### Dataset Format\n","- Multiple test questions\n","- Optional expected responses\n","- Structured for batch processing\n","\n","## Evaluation Workflow\n","\n","### Individual Model Testing\n","```\n","1. Prepare diverse test prompts\n","    ↓\n","2. Test in chat playground (single prompts)\n","    ↓\n","3. Use manual evaluations (multiple prompts)\n","    ↓\n","4. Rate responses (thumbs up/down)\n","    ↓\n","5. Adjust model/parameters\n","    ↓\n","6. Re-evaluate improvements\n","```\n","\n","### Integration with Chat Application\n","```\n","Individual Model Evaluation\n","    ↓\n","Integrate into Prompt Flow\n","    ↓\n","Evaluate Complete Flow\n","(manually or automatically)\n","```\n","\n","## Comparison: Chat Playground vs Manual Evaluations\n","\n","| Feature | Chat Playground | Manual Evaluations |\n","|---------|-----------------|-------------------|\n","| **Speed** | Single prompt testing | Multiple prompts batch |\n","| **Best For** | Early experimentation | Systematic evaluation |\n","| **Dataset** | One prompt at a time | Upload multiple questions |\n","| **Rating** | Subjective observation | Thumbs up/down feature |\n","| **Use Case** | Quick iteration | Comprehensive assessment |\n","\n","## Improvement Strategy\n","\n","### What to Adjust Based on Results\n","\n","**Poor Relevance**:\n","- Modify system message\n","- Adjust input prompts\n","- Add grounding data\n","\n","**Inconsistent Quality**:\n","- Change temperature (lower for consistency)\n","- Refine system message\n","- Test different model\n","\n","**Inappropriate Length**:\n","- Adjust max response parameter\n","- Modify prompt instructions\n","- Update system message\n","\n","**Off-Topic Responses**:\n","- Strengthen system message constraints\n","- Add specific instructions\n","- Consider model fine-tuning\n","\n","## Best Practices\n","\n","### Test Prompt Design\n","- Cover wide range of scenarios\n","- Include challenging cases\n","- Represent real user queries\n","- Test edge cases\n","\n","### Evaluation Process\n","- Start with chat playground\n","- Progress to manual evaluations\n","- Use consistent rating criteria\n","- Document findings\n","\n","### Iterative Improvement\n","- Test baseline first\n","- Make one change at a time\n","- Re-evaluate after each change\n","- Track what works\n","\n","### Parameter Tuning\n","- Start with default settings\n","- Adjust temperature for creativity needs\n","- Set appropriate max response length\n","- Document parameter impacts\n","\n","## From Model to Application\n","\n","### Progression Path\n","1. **Model evaluation**: Test individual model\n","2. **Integration**: Add to prompt flow\n","3. **Flow evaluation**: Test complete application\n","4. **Deployment**: Move to production\n","5. **Ongoing evaluation**: Continue manual testing\n","\n","### Why Test Both\n","- Model may work well individually\n","- Integration can introduce issues\n","- Flow logic affects final output\n","- Complete system needs validation\n","\n","## Key Considerations\n","\n","### Human Insight Value\n","Manual evaluations provide:\n","- Context understanding\n","- Nuanced quality assessment\n","- User satisfaction perspective\n","- Insights automated metrics miss\n","\n","### Scalability Balance\n","- Manual evaluation: Deep insights, limited scale\n","- Automated evaluation: Broad coverage, less nuance\n","- Combined approach: Best of both\n","\n","### Early vs Late Development\n","- **Early**: More manual evaluation, rapid iteration\n","- **Late**: More automated, manual validation\n","- **Production**: Ongoing manual sampling\n","\n","## Key Takeaway\n","Manual evaluation uses chat playground for individual prompt testing with iterative refinement, and manual evaluations feature for batch testing multiple prompts with thumbs up/down ratings. Prepare diverse test prompts covering common cases, edge cases, and failure points. Adjust system message, temperature, and max response parameters based on results, then integrate tested models into prompt flows for complete application evaluation."],"metadata":{"id":"K8P3rkA1_ohD"}},{"cell_type":"markdown","source":["# **Automated evaluations**\n","---\n","\n","## Overview\n","Azure AI Foundry portal enables automated assessment of quality and content safety performance for models, datasets, or prompt flows.\n","\n","## Evaluation Data Requirements\n","\n","### Dataset Components\n","- **Prompts**: Input questions or queries\n","- **Responses**: Generated model outputs\n","- **Ground truth (optional)**: Expected responses for comparison\n","\n","### Three Dataset Creation Methods\n","\n","**1. Manual Compilation**:\n","- Manually create prompts and responses\n","- Time-consuming but controlled\n","- Ensures quality and relevance\n","\n","**2. Existing Application Output**:\n","- Use real application data\n","- Reflects actual usage patterns\n","- Authentic user scenarios\n","\n","**3. AI-Generated (Recommended Starting Point)**:\n","- Use AI model to generate prompt/response sets\n","- Related to specific subject\n","- Edit generated content to reflect desired output\n","- Use as ground truth for evaluating other models\n","\n","### AI-Generated Dataset Workflow\n","```\n","1. Use AI model to generate prompts and responses\n","    ↓\n","2. Edit to reflect desired output\n","    ↓\n","3. Use as ground truth\n","    ↓\n","4. Evaluate other model's responses against this baseline\n","```\n","\n","## Evaluation Metrics Categories\n","\n","### 1. AI Quality Metrics\n","\n","**Purpose**: Measure quality of model responses\n","\n","**Two Measurement Approaches**:\n","\n","#### AI Model-Based Evaluation\n","- Uses AI models to assess responses\n","- Evaluates subjective quality aspects\n","\n","**Key Metrics**:\n","- **Coherence**: Logical flow and readability\n","- **Relevance**: Pertinence to input query\n","\n","#### Standard NLP Metrics\n","- Requires ground truth (expected response text)\n","- Quantifies overlap with expected responses\n","\n","**Key Metrics**:\n","- **F1 Score**: Ratio of shared words between generated and ground truth\n","- **BLEU** (Bilingual Evaluation Understudy): Translation quality, n-gram precision\n","- **METEOR** (Metric for Evaluation of Translation with Explicit Ordering): Translation with synonyms/stemming\n","- **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation): Summarization, n-gram recall\n","\n","### 2. Risk and Safety Metrics\n","\n","**Purpose**: Assess responses for content safety issues\n","\n","**Four Safety Categories**:\n","- **Violence**: Violent content or threats\n","- **Hate**: Discriminatory or hateful content\n","- **Sexual**: Explicit or inappropriate sexual content\n","- **Self-harm**: Content promoting or describing self-harm\n","\n","**Function**: Identify potentially harmful content in model outputs\n","\n","## Evaluator Selection\n","\n","### Customizable Evaluation\n","- Choose specific evaluators\n","- Select relevant metrics\n","- Tailor to use case needs\n","\n","### Quality Evaluators\n","- Select AI-assisted metrics (coherence, relevance)\n","- Choose NLP metrics (F1, BLEU, METEOR, ROUGE)\n","- Combine multiple metrics\n","\n","### Safety Evaluators\n","- Enable violence detection\n","- Enable hate speech detection\n","- Enable sexual content detection\n","- Enable self-harm content detection\n","\n","## Evaluation Targets\n","\n","### Three Evaluation Scopes\n","1. **Models**: Assess individual language model performance\n","2. **Datasets**: Evaluate quality of prompt/response datasets\n","3. **Prompt Flows**: Test complete application flows\n","\n","### Flexibility\n","Automated evaluation works across all three scopes for comprehensive assessment\n","\n","## Automated vs Manual Evaluation\n","\n","### Automated Evaluation Advantages\n","- **Scalable**: Test large datasets\n","- **Consistent**: Apply same criteria uniformly\n","- **Efficient**: Fast processing\n","- **Quantitative**: Objective metrics\n","- **Repeatable**: Consistent results\n","\n","### When to Use Automated\n","- Large-scale testing\n","- Baseline establishment\n","- Continuous monitoring\n","- Objective comparison\n","- Ground truth available\n","\n","### Combined Approach\n","- Automated for scale and consistency\n","- Manual for nuanced insights\n","- Together provide comprehensive assessment\n","\n","## Evaluation Process\n","\n","### Setup Phase\n","```\n","1. Prepare evaluation dataset\n","   (prompts + responses + optional ground truth)\n","    ↓\n","2. Select evaluators\n","   (quality metrics + safety metrics)\n","    ↓\n","3. Configure evaluation settings\n","    ↓\n","4. Run evaluation\n","```\n","\n","### Analysis Phase\n","```\n","1. Review quality metrics\n","    ↓\n","2. Review safety metrics\n","    ↓\n","3. Identify issues and patterns\n","    ↓\n","4. Implement improvements\n","    ↓\n","5. Re-evaluate\n","```\n","\n","## Ground Truth Importance\n","\n","### Metrics Requiring Ground Truth\n","- F1 Score\n","- BLEU\n","- METEOR\n","- ROUGE\n","\n","### Metrics Not Requiring Ground Truth\n","- Coherence (AI-assisted)\n","- Relevance (AI-assisted)\n","- Risk and safety metrics\n","\n","### Creating Effective Ground Truth\n","- Representative of desired outputs\n","- High quality and accurate\n","- Cover diverse scenarios\n","- Regularly updated\n","\n","## Best Practices\n","\n","### Dataset Preparation\n","- Include diverse test cases\n","- Ensure ground truth quality\n","- Cover edge cases\n","- Represent real usage\n","\n","### Evaluator Selection\n","- Choose relevant metrics for use case\n","- Balance quality and safety evaluators\n","- Consider metric limitations\n","- Use multiple metrics\n","\n","### Evaluation Frequency\n","- After model changes\n","- Before deployment\n","- Regularly in production\n","- When issues detected\n","\n","### Results Interpretation\n","- Compare against baseline\n","- Look for patterns\n","- Consider metric combinations\n","- Validate with manual review\n","\n","## Metric Selection Guide\n","\n","| Use Case | Recommended Metrics |\n","|----------|-------------------|\n","| **Text Generation** | Coherence, Relevance, F1 Score |\n","| **Translation** | BLEU, METEOR |\n","| **Summarization** | ROUGE, Coherence |\n","| **Safety-Critical** | All risk and safety metrics |\n","| **Customer-Facing** | Quality + safety metrics |\n","\n","## Key Takeaway\n","Automated evaluation in Azure AI Foundry assesses models, datasets, or prompt flows using two metric categories: AI Quality (coherence, relevance, F1, BLEU, METEOR, ROUGE) and Risk and Safety (violence, hate, sexual, self-harm). Create evaluation datasets through manual compilation, existing app output, or AI-generated content edited as ground truth. Select appropriate evaluators based on use case needs for scalable, consistent, and objective assessment."],"metadata":{"id":"bPyhKdJwHzau"}},{"cell_type":"markdown","source":["# **Quiz**\n","---\n","# AI-102 Study Notes: Module Assessment 8 - Evaluation\n","\n","## Question 1: Human Judgment Evaluation\n","**Question**: Which evaluation technique can you use to apply your own judgement about the quality of responses to a set of specific prompts?\n","\n","**Correct Answer**: Manual evaluations\n","\n","**Explanation**: Manual evaluations involve human raters applying their judgment to assess response quality, capturing insights that automated metrics might miss (context relevance, user satisfaction).\n","\n","**Wrong Answers**:\n","- ❌ Model benchmarks: Publicly available automated metrics for comparing models, not human judgment\n","- ❌ Automated evaluations: Use AI or NLP metrics, not direct human judgment\n","\n","## Question 2: Ground Truth Comparison\n","**Question**: Which evaluator compares generated responses to ground truth based on standard metrics?\n","\n","**Correct Answer**: F1 Score\n","\n","**Explanation**: F1 Score is a standard NLP metric that measures the ratio of shared words between generated responses and ground truth answers, requiring expected responses for comparison.\n","\n","**Wrong Answers**:\n","- ❌ Coherence: AI-assisted metric evaluating logical flow, doesn't require ground truth\n","- ❌ Protected material: Safety metric for copyright detection, not ground truth comparison\n","\n","## Question 3: AI-Assisted Structure Evaluation\n","**Question**: Which evaluator metric uses an AI model to judge the structure and logical flow of ideas in a response?\n","\n","**Correct Answer**: Coherence\n","\n","**Explanation**: Coherence is an AI-assisted metric that uses AI models to evaluate whether output flows smoothly, reads naturally, and has logical structure.\n","\n","**Wrong Answers**:\n","- ❌ F1 Score: Standard NLP metric measuring word overlap, not AI-assisted structure evaluation\n","- ❌ Protected material: Safety metric for copyright, not structure evaluation\n"],"metadata":{"id":"km1zMMLlJxyl"}},{"cell_type":"markdown","source":["# **Code Exercise**\n","---\n","\n","## Overview\n","General guide for manual and automated evaluation of generative AI models in Azure AI Foundry portal\n","\n","## Required Setup\n","\n","### Model Deployment Strategy\n","Deploy two models with distinct purposes:\n","1. **Evaluation model**: Higher-capability model (e.g., GPT-4) for AI-assisted evaluation\n","2. **Test model**: Model being evaluated (any deployed model)\n","\n","**Standard Deployment Settings**:\n","- Deployment type: Global Standard or appropriate type\n","- TPM: Configure based on expected load\n","- Content filter: Apply appropriate filter\n","\n","## Manual Evaluation Guide\n","\n","### 1. Prepare Evaluation Dataset\n","**Format**: JSONL file containing:\n","- Input questions/prompts\n","- Expected responses (optional ground truth)\n","\n","**Dataset Structure**:\n","```json\n","{\"question\": \"Your prompt\", \"ExpectedResponse\": \"Expected output\"}\n","```\n","\n","### 2. Navigate to Manual Evaluation\n","**Path**: Protect and govern → Evaluation → Manual evaluations → New manual evaluation\n","\n","### 3. Configure Model Settings\n","- **Select model**: Choose model to evaluate\n","- **System message**: Define model behavior and constraints\n","- **Context**: Add any necessary grounding instructions\n","\n","### 4. Import and Map Test Data\n","- Upload JSONL dataset file\n","- Map dataset fields to evaluation components:\n","  - **Input field** → Questions/prompts\n","  - **Expected response field** → Ground truth (if available)\n","\n","### 5. Generate and Score Responses\n","- **Run evaluation**: Generate outputs for all inputs\n","- **Review outputs**: Compare to expected responses\n","- **Score responses**: Use thumbs up/down for each output\n","- **Track patterns**: Note recurring issues or successes\n","\n","### 6. Save and Document Results\n","- **Save results**: Assign descriptive name\n","- **Document findings**: Note observations and insights\n","- **Enable comparison**: Use for future model comparisons\n","\n","### Manual Evaluation Use Cases\n","✓ Initial model assessment\n","✓ Small-scale testing\n","✓ Subjective quality judgment\n","✓ Human perspective validation\n","\n","## Automated Evaluation Guide\n","\n","### 1. Select Evaluation Type\n","**Path**: Evaluation → Automated evaluations → Create new evaluation\n","**Option**: Choose \"Evaluate a model\"\n","\n","### 2. Configure Data Source\n","- Select or upload evaluation dataset\n","- Ensure dataset includes questions and expected responses\n","- Verify dataset format compatibility\n","\n","### 3. Configure Test Model\n","- **Select model**: Model to be evaluated\n","- **System message**: Define behavior (use consistent message for fair comparison)\n","- **Query mapping**: Map to dataset question field (e.g., {{item.question}})\n","\n","### 4. Select and Configure Evaluators\n","\n","#### Quality Evaluators\n","\n","**AI-Assisted Quality Metrics**:\n","- **Semantic Similarity**: Compare meaning with ground truth\n","  - Grade with: Evaluation model (e.g., GPT-4)\n","  - Map: Output and ground truth fields\n","  \n","- **Relevance**: Assess response pertinence to query\n","  - Grade with: Evaluation model\n","  - Map: Query field\n","\n","**Standard NLP Metrics**:\n","- **F1 Score**: Word overlap measurement\n","  - Requires: Ground truth field mapping\n","  - No grading model needed\n","\n","#### Safety Evaluators\n","\n","**Content Safety Metrics**:\n","- **Hate and Unfairness**: Detect discriminatory content\n","- **Violence**: Detect violent content\n","- **Sexual Content**: Detect inappropriate sexual content\n","- **Self-Harm**: Detect self-harm related content\n","\n","Configure with:\n","- Query field mapping\n","- Optional output field mapping\n","\n","### 5. Field Mapping Patterns\n","\n","**Standard Mapping Syntax**:\n","```\n","{{item.fieldname}}      // Dataset field\n","{{sample.output_text}}  // Generated model output\n","```\n","\n","**Common Configurations**:\n","- Query: {{item.question}} or {{item.query}}\n","- Ground Truth: {{item.ExpectedResponse}} or {{item.expected}}\n","- Output: {{sample.output_text}}\n","\n","### 6. Submit and Monitor\n","- **Name evaluation**: Use descriptive, version-tracked naming\n","- **Submit**: Start evaluation process\n","- **Monitor**: Check status with refresh\n","- **Wait**: Allow completion (duration varies by dataset size)\n","\n","### 7. Analyze Results\n","\n","**Metrics Tab**:\n","- Review aggregate scores\n","- Compare across evaluators\n","- Identify overall performance\n","\n","**Data Tab**:\n","- Examine individual results\n","- Review reasoning explanations\n","- Identify specific failure patterns\n","- Extract insights for improvement\n","\n","## Evaluation Strategy\n","\n","### When to Use Manual Evaluation\n","- **Early development**: Quick testing and iteration\n","- **Small datasets**: Limited test cases (<50 prompts)\n","- **Subjective assessment**: Context and satisfaction matters\n","- **Exploratory testing**: Understanding model behavior\n","\n","### When to Use Automated Evaluation\n","- **Large datasets**: Many test cases (50+ prompts)\n","- **Standardized metrics**: Need objective measurements\n","- **Continuous monitoring**: Regular performance tracking\n","- **Comparison testing**: Evaluating multiple models/configurations\n","\n","### Combined Approach\n","1. **Start**: Manual evaluation for initial assessment\n","2. **Scale**: Automated evaluation for comprehensive testing\n","3. **Validate**: Periodic manual review of automated results\n","4. **Iterate**: Use both for continuous improvement\n","\n","## Evaluator Selection Guide\n","\n","### For Quality Assessment\n","**Include**:\n","- Semantic Similarity (AI-assisted with evaluation model)\n","- Relevance (AI-assisted with evaluation model)\n","- F1 Score (standard NLP, requires ground truth)\n","\n","### For Safety Assessment\n","**Include**:\n","- Hate and Unfairness\n","- Violence (if relevant to use case)\n","- Sexual Content (if relevant to use case)\n","- Self-Harm (if relevant to use case)\n","\n","### For Translation/Summarization\n","**Add**:\n","- BLEU score (translation)\n","- ROUGE score (summarization)\n","- METEOR score (translation with ordering)\n","\n","## System Message Best Practices\n","\n","### Consistency Principle\n","Use **identical system message** across:\n","- Manual evaluations\n","- Automated evaluations\n","- Different model comparisons\n","\n","**Benefit**: Ensures fair, controlled comparison\n","\n","### Effective System Messages\n","- **Clear role definition**: Specify AI assistant purpose\n","- **Behavioral constraints**: Define what to do/not do\n","- **Tone guidance**: Specify response style\n","- **Scope boundaries**: Limit subject matter\n","\n","**Example Template**:\n","```\n","You are a [role] that helps users with [task].\n","Your objective is to [primary goal].\n","You should [dos] and should not [don'ts].\n","```\n","\n","## Dataset Preparation Guidelines\n","\n","### Quality Requirements\n","- **Representative**: Cover real-world scenarios\n","- **Diverse**: Include various question types\n","- **Balanced**: Mix difficulty levels\n","- **Edge cases**: Include boundary conditions\n","\n","### Ground Truth Guidelines\n","- **Accurate**: Correct expected responses\n","- **Consistent**: Follow same style/format\n","- **Complete**: Cover all test scenarios\n","- **Reviewed**: Validated by subject matter experts\n","\n","## Comparison Workflow\n","\n","### Multi-Model Evaluation\n","```\n","1. Baseline: Evaluate first model\n","    ↓\n","2. Save results with version/name\n","    ↓\n","3. Configure: Evaluate second model (same dataset, system message)\n","    ↓\n","4. Compare: Review metrics side-by-side\n","    ↓\n","5. Decide: Select best-performing model\n","```\n","\n","### Configuration Testing\n","```\n","1. Test: Base configuration\n","    ↓\n","2. Change: One parameter (temperature, system message, etc.)\n","    ↓\n","3. Re-evaluate: Same dataset\n","    ↓\n","4. Compare: Identify improvements\n","    ↓\n","5. Iterate: Optimize configuration\n","```\n","\n","## Troubleshooting Common Issues\n","\n","### Missing Metrics\n","**Cause**: Incomplete field mapping\n","\n","**Solution**: Verify all required fields mapped correctly\n","\n","### Low Scores Across All Metrics\n","**Cause**: System message or model mismatch\n","\n","**Solution**: Review and refine system message\n","\n","### Evaluation Timeout\n","**Cause**: Dataset too large or rate limits\n","\n","**Solution**: Reduce dataset size or increase TPM\n","\n","### Inconsistent Results\n","**Cause**: Non-deterministic model behavior\n","\n","**Solution**: Lower temperature parameter for consistency\n","\n","## Key Takeaway\n","Manual evaluation provides human-scored assessment through thumbs up/down, ideal for small-scale subjective testing. Automated evaluation offers scalable, standardized metrics through multiple evaluator types (AI-assisted quality, standard NLP, safety metrics). Use consistent system messages, proper field mapping ({{item.field}} syntax), and appropriate evaluator selection based on assessment goals. Combine both approaches for comprehensive model evaluation."],"metadata":{"id":"N1ZY-yQOKMO1"}}]}