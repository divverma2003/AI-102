{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDq8ZLjfLzthiTlEV4bSSN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Introduction**\n","---\n","\n","## Pretrained Models Benefits\n","- Great starting point for development\n","- Save time and effort\n","- Require less data for specific use cases\n","\n","## Business Use Case Example\n","**Scenario**: Travel agency chat application\n","\n","**Requirements**:\n","- Specific response format and style\n","- Company-specific tone of voice\n","- Marketing alignment with brand voice\n","\n","## Fine-Tuning vs Training From Scratch\n","\n","### Fine-Tuning Benefits\n","- **Less time** required\n","- **Fewer compute resources** needed\n","- **Less data** required for customization\n","- Builds on existing pretrained model capabilities\n","\n","### When to Fine-Tune\n","- Need specific response format or style\n","- Want consistent tone of voice\n","- Require domain-specific behavior\n","- Have limited training data or resources\n","\n","## Implementation\n","Fine-tune base models from Azure AI Foundry model catalog → integrate with chat applications\n","\n","## Key Takeaway\n","Fine-tuning customizes pretrained language models to specific needs with less time, data, and compute resources compared to training from scratch, making it ideal for businesses needing specialized response formats and company-specific tone of voice."],"metadata":{"id":"Im0cKy3wbJD3"}},{"cell_type":"markdown","source":["# **Understand when to fine-tune a language model**\n","---\n","\n","## Model Optimization Strategies\n","\n","### Three Main Approaches\n","1. **Prompt Engineering**: Quick and easy way to improve model behavior\n","2. **Retrieval Augmented Generation (RAG)**: Ground responses in specific data\n","3. **Fine-Tuning**: Train base model on custom dataset\n","\n","## Strategy Selection Framework\n","\n","### Context Optimization (What the model needs to know)\n","**Use**: RAG and Prompt Engineering\n","\n","**Goal**: Provide factual, grounded information\n","\n","**Example**: Travel booking catalog with hotel information\n","\n","### Model Optimization (How the model needs to act)\n","**Use**: Fine-tuning (with or without other strategies)\n","\n","**Goal**: Consistent style, format, and tone\n","\n","**Example**: Company-specific tone of voice\n","\n","## Prompt Engineering\n","\n","### Capabilities\n","- Change question formatting\n","- Update system messages\n","- Quick implementation\n","\n","### Limitations\n","- May not lead to consistent results\n","- Model might ignore instructions\n","- Behavior can vary\n","\n","### Few-Shot Learning\n","**Technique**: Provide examples of desired output\n","- **One-shot**: Single example\n","- **Few-shot**: Multiple examples\n","\n","**Issue**: Model doesn't always follow specified format despite examples\n","\n","## When to Use RAG\n","\n","### Best For\n","- Factual responses required\n","- Grounding in specific data sources\n","- Domain-specific knowledge bases\n","- Real-time or updated information\n","\n","### Example Use Case\n","Customer questions about available hotels in travel catalog\n","\n","## When to Fine-Tune\n","\n","### Best For\n","- Specific style and format requirements\n","- Consistent tone of voice needed\n","- Behavioral consistency required\n","- Prompt engineering alone insufficient\n","\n","### Key Advantage\n","**Maximizes consistency** of model behavior across all responses\n","\n","## Combined Strategies\n","\n","### Optimization Combinations\n","Can combine multiple approaches:\n","- RAG + Fine-tuning\n","- Prompt Engineering + RAG\n","- Prompt Engineering + Fine-tuning\n","- All three strategies together\n","\n","### Decision Matrix\n","\n","| Need | Primary Strategy | Secondary Strategy |\n","|------|-----------------|-------------------|\n","| Factual accuracy | RAG | Prompt Engineering |\n","| Consistent behavior | Fine-tuning | Prompt Engineering |\n","| Specific knowledge + style | Fine-tuning | RAG |\n","| Quick improvements | Prompt Engineering | None |\n","\n","## Implementation Order\n","\n","### Recommended Approach\n","1. **Start**: Prompt Engineering (fastest, easiest)\n","2. **Add context**: RAG (if factual grounding needed)\n","3. **Ensure consistency**: Fine-tuning (if behavioral consistency required)\n","\n","## Key Takeaway\n","Fine-tuning is best used when you need consistent model behavior regarding style, format, and tone that prompt engineering alone cannot achieve. RAG addresses what the model knows (context), while fine-tuning addresses how the model acts (behavior). Strategies can be combined for optimal results."],"metadata":{"id":"KJv9T-IKcWE2"}},{"cell_type":"markdown","source":["# **Prepare your data to fine-tune a chat completion model**\n","---\n","\n","## Dataset Requirements\n","\n","### Data Quality Impact\n","- Dataset quality directly affects model quality\n","- Less data needed than training from scratch\n","- Still need enough data for behavioral consistency\n","- Amount needed depends on use case\n","\n","### Three Required Components\n","1. **System message**: Defines AI behavior and role\n","2. **User message**: Example user input/question\n","3. **Assistant's response**: Model's ideal response\n","\n","## JSONL File Format\n","\n","### Single-Turn Example\n","```json\n","{\n","  \"messages\": [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox.\"\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"Is Xbox better than PlayStation?\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?\"\n","    }\n","  ]\n","}\n","```\n","\n","### Multi-Turn Example with Weights\n","```json\n","{\n","  \"messages\": [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"What's the capital of France?\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"Paris\",\n","      \"weight\": 0\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"Can you be more sarcastic?\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"Paris, as if everyone doesn't know that already.\",\n","      \"weight\": 1\n","    }\n","  ]\n","}\n","```\n","\n","## Weight Parameter Details\n","\n","### Purpose\n","Control which assistant messages are used for training fine-tuned model\n","\n","### Weight Values\n","\n","**weight: 0**\n","- Message is **ignored** during training\n","- Not used to update model parameters\n","- Use when: Response is correct but not ideal example of desired behavior\n","\n","**weight: 1**\n","- Message is **included** for training\n","- Used to update model parameters\n","- Use when: Response represents ideal behavior you want model to learn\n","\n","### Use Cases for Weights\n","\n","**Selective Training**:\n","- Include only specific turns in multi-turn conversations\n","- Focus training on particular response styles\n","- Filter out acceptable but non-exemplary responses\n","\n","**Training Efficiency**:\n","- Reduce training on redundant patterns\n","- Emphasize important behavioral examples\n","- Control learning from specific interaction types\n","\n","**Quality Control**:\n","- Exclude responses that are correct but lack desired tone\n","- Include only responses that perfectly match target behavior\n","- Fine-tune specific aspects of model behavior\n","\n","### Strategic Weight Usage\n","- **Initial response (weight: 0)**: Basic factual answer, not showcasing desired style\n","- **Follow-up response (weight: 1)**: Enhanced response demonstrating target behavior (sarcasm, specific tone)\n","- **Result**: Model learns the enhanced behavior while maintaining factual accuracy\n","\n","## Dataset Creation Best Practices\n","\n","### Data Sources\n","- Real chat application history\n","- Curated example conversations\n","- Manually created ideal interactions\n","\n","### Data Preparation Requirements\n","\n","**Remove Sensitive Information**:\n","- Personal identifiable information (PII)\n","- Confidential business data\n","- Private customer details\n","\n","**Ensure Diversity**:\n","- Various topics and scenarios\n","- Different conversation styles\n","- Multiple turn lengths\n","- Edge cases and exceptions\n","\n","**Quality Over Quantity**:\n","- High-quality examples more valuable than large datasets\n","- Each example should demonstrate ideal behavior\n","- Diverse examples prevent overfitting\n","\n","### Multi-Turn Conversations\n","- Can include multiple conversation turns on single line\n","- Represents realistic conversation flow\n","- Shows context handling and follow-up responses\n","\n","## Dataset Preparation Steps\n","\n","1. **Understand desired model behaviors**: Define target style, format, tone\n","2. **Create JSONL format dataset**: Structure with system, user, assistant messages\n","3. **Ensure high quality**: Include only ideal behavioral examples\n","4. **Add diversity**: Cover various scenarios and conversation types\n","5. **Apply weights strategically**: Control which responses train the model\n","6. **Remove sensitive data**: Protect privacy and confidentiality\n","\n","## Key Takeaway\n","Fine-tuning datasets use JSONL format with system, user, and assistant messages. The optional weight parameter (0=ignore, 1=include) provides granular control over which assistant responses train the model, enabling selective learning of specific behaviors while maintaining conversation context.\n"],"metadata":{"id":"wQm7R692f53h"}},{"cell_type":"markdown","source":["# **Explore fine-tuning language models in Azure AI Foundry portal**\n","---\n","\n","## Foundation Model Selection\n","\n","### Available Through Azure AI Foundry Model Catalog\n","- Pretrained on large amounts of data\n","- Can fine-tune for various tasks: text classification, translation, chat completion\n","\n","### Popular Chat Completion Models\n","- GPT-4\n","- Llama-2-7b\n","\n","### Regional Availability Considerations\n","**Important**: Not all models available in all regions due to quota limitations\n","- Verify model availability in your AI hub region before starting\n","- Check quota allocation for desired model\n","\n","## Model Selection Criteria\n","\n","### Model Capabilities\n","- Evaluate how well capabilities align with task\n","- Example: BERT better for understanding short texts\n","\n","### Pretraining Data\n","- Consider dataset used for pretraining\n","- Example: GPT-2 trained on unfiltered internet content (potential biases)\n","\n","### Limitations and Biases\n","- Be aware of inherent model limitations\n","- Understand potential biases in pretrained models\n","\n","### Language Support\n","- Check for specific language requirements\n","- Verify multilingual capabilities if needed\n","\n","### Model Cards\n","- Descriptions available in Azure AI Foundry portal\n","- Detailed information on Hugging Face website\n","- Referenced in model overview\n","\n","## Fine-Tuning Job Configuration\n","\n","### Four Configuration Steps\n","1. Select base model\n","2. Select training data\n","3. (Optional) Select validation data\n","4. Configure advanced options\n","\n","## Advanced Training Options\n","\n","### batch_size\n","**Definition**: Number of training examples used in single forward and backward pass\n","\n","**Considerations**:\n","- Larger batch sizes generally work better for larger datasets\n","- Default and maximum values are model-specific\n","- Larger batch size = less frequent parameter updates with lower variance\n","\n","**Trade-offs**: Update frequency vs. stability\n","\n","### learning_rate_multiplier\n","**Definition**: Fine-tuning learning rate = original pretraining learning rate × this multiplier\n","\n","**Recommendations**:\n","- Larger learning rates work better with larger batch sizes\n","- Experiment with range 0.02 to 0.2\n","- Smaller learning rate helps avoid overfitting\n","\n","**Use Cases**: Balance between training speed and avoiding overfitting\n","\n","### n_epochs\n","**Definition**: Number of complete cycles through entire training dataset\n","\n","**Purpose**: Controls how many times model sees all training data\n","\n","**Consideration**: More epochs can improve learning but risk overfitting\n","\n","### seed\n","**Definition**: Controls reproducibility of training job\n","\n","**Characteristics**:\n","- Same seed + same parameters = same results (usually)\n","- Rare cases may differ\n","- Auto-generated if not specified\n","\n","**Use Case**: Ensure reproducible experiments\n","\n","## Training Job Lifecycle\n","\n","### Job Submission\n","1. Submit fine-tuning job with configuration\n","2. Job created to train model\n","3. Monitor status during training\n","\n","### Job Completion\n","- Review input parameters\n","- Understand how fine-tuned model was created\n","- Examine training configuration used\n","\n","### Validation (Optional)\n","- If validation dataset included\n","- Review model performance metrics\n","- Assess generalization on validation data\n","\n","## Model Deployment and Testing\n","\n","### Deployment Options\n","- Can always deploy fine-tuned model\n","- Creates endpoint for model access\n","\n","### Testing Process\n","1. Deploy fine-tuned model\n","2. Test model performance\n","3. Assess quality of responses\n","4. Verify desired behavior\n","\n","### Integration\n","- Once satisfied with performance\n","- Integrate deployed model with chat application\n","- Use model endpoint in application code\n","\n","## Complete Workflow\n","\n","```\n","Select Base Model\n","    ↓\n","Configure Training Data\n","    ↓\n","(Optional) Add Validation Data\n","    ↓\n","Set Advanced Options\n","    ↓\n","Submit Fine-Tuning Job\n","    ↓\n","Monitor Training Status\n","    ↓\n","Review Performance (if validation data)\n","    ↓\n","Deploy Model\n","    ↓\n","Test Model\n","    ↓\n","Integrate with Application\n","```\n","\n","## Best Practices\n","\n","### Model Selection\n","- Filter by fine-tuning task in catalog\n","- Review model cards on Hugging Face\n","- Consider regional availability and quota\n","- Evaluate alignment with use case\n","\n","### Configuration\n","- Start with default values for advanced options\n","- Experiment with learning rate (0.02-0.2 range)\n","- Balance batch size with dataset size\n","- Use validation data to assess performance\n","\n","### Testing and Deployment\n","- Always test before production integration\n","- Review validation metrics if available\n","- Verify behavior matches desired outcomes\n","- Monitor performance after deployment\n","\n","## Key Takeaway\n","Fine-tuning in Azure AI Foundry involves selecting an appropriate base model from the catalog, configuring training with your JSONL dataset, setting advanced options (batch_size, learning_rate_multiplier, n_epochs, seed), monitoring the training job, and deploying the model for testing and integration with applications."],"metadata":{"id":"rHovFLgHibGE"}},{"cell_type":"markdown","source":["# **Quiz**\n","---\n","## Question 1: Data Format for Fine-Tuning\n","**Question**: How must data be formatted for fine-tuning?\n","\n","**Correct Answer**: JSONL\n","\n","**Explanation**: JSONL (JSON Lines) format with system, user, and assistant messages is required for fine-tuning language models.\n","\n","**Wrong Answers**:\n","- ❌ YAML: Not a supported format for fine-tuning\n","- ❌ HTML: Web markup language, not training data format\n","\n","## Question 2: Fine-Tuning Purpose\n","**Question**: What does fine-tuning optimize in your model?\n","\n","**Correct Answer**: How the model needs to act\n","\n","**Explanation**: Fine-tuning optimizes model behavior - style, format, tone, and consistency of responses.\n","\n","**Wrong Answers**:\n","- ❌ What the model needs to know: This is addressed by RAG (Retrieval Augmented Generation)\n","- ❌ Which words aren't allowed: Content filtering, not fine-tuning purpose\n","\n","## Question 3: Training Cycle Parameter\n","**Question**: Which advanced option refers to one full cycle through the training dataset?\n","\n","**Correct Answer**: n_epochs\n","\n","**Explanation**: An epoch is one complete pass through the entire training dataset.\n","\n","**Wrong Answers**:\n","- ❌ seed: Controls reproducibility of training job\n","- ❌ batch_size: Number of examples in single forward/backward pass\n","\n","## Key Patterns\n","\n","**JSONL Format** = Required data structure for fine-tuning\n","**Fine-tuning** = Optimizes how model acts (behavior)\n","**n_epochs** = Number of complete training cycles\n","\n","## Optimization Framework\n","\n","| Optimization Type | Strategy | Purpose |\n","|------------------|----------|---------|\n","| **What model knows** | RAG | Context and factual data |\n","| **How model acts** | Fine-tuning | Style, format, tone |\n","\n","## Quick Reference\n","\n","| Parameter | Definition |\n","|-----------|------------|\n","| **JSONL** | JSON Lines format with system/user/assistant messages |\n","| **n_epochs** | Complete cycles through training dataset |\n","| **batch_size** | Examples per training pass |\n","| **seed** | Reproducibility control |"],"metadata":{"id":"EmTOD0CEnQT3"}},{"cell_type":"markdown","source":["# **Code Exercise**\n","---\n","\n","## Lab Objective\n","Compare fine-tuned model vs base model to evaluate which better fits specific behavioral requirements (consistent, friendly conversational tone for travel agency chat app)\n","\n","## Supported Regions for Fine-Tuning\n","- East US 2\n","- North Central US\n","- Sweden Central\n","\n","*Note: At time of writing, these regions support fine-tuning for gpt-4o models*\n","\n","## Initial Model Deployment\n","\n","### Base Model Setup\n","- **Model**: gpt-4o\n","- **Deployment type**: Global standard\n","- **TPM limit**: 50K (or maximum available)\n","- **Purpose**: Testing baseline before fine-tuning\n","\n","## Fine-Tuning Configuration\n","\n","### Dataset Requirements\n","- **Format**: JSONL file\n","- **Content**: Training data with system/user/assistant messages\n","- **Source**: Download training dataset (travel-finetune-hotel.jsonl)\n","\n","### Fine-Tuning Settings\n","- **Method**: Supervised\n","- **Base model**: gpt-4o (default version)\n","- **Training data**: Upload JSONL file\n","- **Model suffix**: ft-travel (or custom name)\n","- **Seed**: Random\n","- **Duration**: 30+ minutes (variable based on resources)\n","\n","### Monitoring Progress\n","- View Fine-tuning page under Build and customize\n","- Check job status and logs\n","- Use Refresh button to update view\n","\n","## Base Model Testing (While Waiting)\n","\n","### Test Scenarios\n","\n","**1. Generic Test**\n","- Prompt: \"What can you do?\"\n","- Expected: Generic responses\n","\n","**2. With Basic System Message**\n","```\n","You are an AI assistant that helps people plan their travel.\n","```\n","- Result: May offer hotel/flight/car bookings (undesired behavior)\n","\n","**3. With Improved System Message**\n","```\n","You are an AI travel assistant that helps people plan their trips. Your objective is to offer support for travel-related inquiries, such as visa requirements, weather forecasts, local attractions, and cultural norms.\n","You should not provide any hotel, flight, rental car or restaurant recommendations.\n","Ask engaging questions to help someone plan their trip and think about what they want to do on their holiday.\n","```\n","\n","### Test Questions\n","1. \"Where in Rome should I stay?\"\n","2. \"I'm mostly there for the food. Where should I stay to be within walking distance of affordable restaurants?\"\n","3. \"What are some local delicacies I should try?\"\n","4. \"When is the best time of year to visit in terms of the weather?\"\n","5. \"What's the best way to get around the city?\"\n","\n","**Observation**: Note tone and writing style consistency\n","\n","## Training Data Structure\n","\n","### JSONL Format Example\n","```json\n","{\n","  \"messages\": [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"You are an AI travel assistant that helps people plan their trips. Your objective is to offer support for travel-related inquiries, such as visa requirements, weather forecasts, local attractions, and cultural norms. You should not provide any hotel, flight, rental car or restaurant recommendations. Ask engaging questions to help someone plan their trip and think about what they want to do on their holiday.\"\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"What's a must-see in Paris?\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"Oh la la! You simply must twirl around the Eiffel Tower and snap a chic selfie! After that, consider visiting the Louvre Museum to see the Mona Lisa and other masterpieces. What type of attractions are you most interested in?\"\n","    }\n","  ]\n","}\n","```\n","\n","### Key Components\n","- **System message**: Same instruction set across all examples\n","- **User content**: Travel-related queries\n","- **Assistant content**: Responses demonstrating desired style and tone\n","- **Purpose**: Train model on specific conversational style\n","\n","## Fine-Tuned Model Deployment\n","\n","### When Fine-Tuning Completes\n","1. Navigate to Fine-tuning page\n","2. Select fine-tuning job to view details\n","3. Review Metrics tab for fine-tune metrics\n","\n","### Deployment Configuration\n","- **Deployment name**: Custom valid name\n","- **Deployment type**: Standard\n","- **TPM limit**: 50K (or maximum available)\n","- **Content filter**: Default\n","- **Provisioning**: Wait until state shows \"succeeded\"\n","\n","## Testing Fine-Tuned Model\n","\n","### Setup\n","1. Open fine-tuned model in playground\n","2. Verify system message matches training data instructions\n","3. Use same test questions as base model\n","\n","### Comparison Criteria\n","- **Consistency**: Does model maintain desired behavior?\n","- **Tone**: Is conversational style aligned with training examples?\n","- **Format**: Are responses structured as intended?\n","- **Accuracy**: Does model avoid unwanted recommendations?\n","\n","### Expected Improvements\n","- More consistent adherence to system instructions\n","- Better tone matching training examples\n","- Reduced instances of ignoring instructions\n","- More predictable response patterns\n","\n","## Key Evaluation Points\n","\n","### Base Model Characteristics\n","- Responds to system messages\n","- May inconsistently follow instructions\n","- Generic conversational style\n","- Variable behavior across similar prompts\n","\n","### Fine-Tuned Model Advantages\n","- **Consistency**: More reliable behavior\n","- **Style**: Matches training data examples\n","- **Format**: Adheres to desired structure\n","- **Tone**: Reflects training examples' personality\n","\n","## Workflow Summary\n","\n","```\n","1. Deploy base gpt-4o model\n","    ↓\n","2. Test base model with system messages\n","    ↓\n","3. Start fine-tuning job (30+ min)\n","    ↓\n","4. Review training data structure\n","    ↓\n","5. Monitor fine-tuning progress\n","    ↓\n","6. Deploy fine-tuned model\n","    ↓\n","7. Test fine-tuned model\n","    ↓\n","8. Compare base vs fine-tuned results\n","```\n","\n","## Key Takeaway\n","Fine-tuning provides more consistent behavioral patterns than prompt engineering alone. Training data with system/user/assistant examples teaches the model specific conversational styles and tones. Compare base and fine-tuned models using identical test scenarios to evaluate improvement in consistency and adherence to desired behavior."],"metadata":{"id":"OPYUDJIBXK0u"}}]}