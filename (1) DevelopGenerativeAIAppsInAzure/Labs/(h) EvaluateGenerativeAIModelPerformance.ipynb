{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206efa10",
   "metadata": {},
   "source": [
    "# Evaluate Generative AI Model Performance\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "Evaluating generative AI models is essential to ensure they meet quality, relevance, and Responsible AI expectations. Microsoft Foundry supports both **manual** and **automated evaluations**, enabling you to compare models, prompts, and configurations using consistent metrics.\n",
    "\n",
    "**Estimated time:** ~30 minutes\n",
    "**Status:** Some features are preview / in active development\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts (AI-102 Focus)\n",
    "\n",
    "* **Manual evaluation:** Human judgment against expected responses\n",
    "* **Automated evaluation:** Metric-based and AI-assisted scoring\n",
    "* **Evaluators:** Tools that score responses (semantic similarity, relevance, safety)\n",
    "* **Ground truth:** Expected answer used for comparison\n",
    "* **Model-as-a-judge:** Using a stronger model to score another model\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Create a Foundry Hub and Project\n",
    "\n",
    "> Evaluation features require a **Foundry hub–based project**.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Open **[https://ai.azure.com](https://ai.azure.com)** and sign in\n",
    "2. Navigate to:\n",
    "\n",
    "   * **Management Center → All resources → Create new**\n",
    "3. Select **Create new AI hub resource**\n",
    "\n",
    "### Project Configuration\n",
    "\n",
    "* **Project name:** Valid name\n",
    "* **Hub:** Create new → Rename to a unique alphanumeric value\n",
    "* **Advanced options:**\n",
    "\n",
    "  * **Subscription:** Your Azure subscription\n",
    "  * **Resource group:** Create or select\n",
    "  * **Region:**\n",
    "\n",
    "    * East US 2\n",
    "    * France Central\n",
    "    * UK South\n",
    "    * Sweden Central\n",
    "\n",
    "**Tip:** If *Create* is disabled, rename the hub.\n",
    "\n",
    "Wait for project creation to complete.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Deploy Models\n",
    "\n",
    "You will deploy **two models**:\n",
    "\n",
    "| Model       | Purpose                              |\n",
    "| ----------- | ------------------------------------ |\n",
    "| gpt-4o      | AI-assisted evaluation (judge model) |\n",
    "| gpt-4o-mini | Model being evaluated                |\n",
    "\n",
    "---\n",
    "\n",
    "### Deploy gpt-4o\n",
    "\n",
    "1. In the project navigation pane, select **Models + endpoints**\n",
    "2. Open **Model deployments** tab\n",
    "3. Select **+ Deploy model → Deploy base model**\n",
    "4. Search for **gpt-4o** and confirm\n",
    "\n",
    "**Deployment settings:**\n",
    "\n",
    "* Deployment type: Global Standard\n",
    "* Automatic version update: Enabled\n",
    "* Model version: Most recent\n",
    "* Connected AI resource: Azure OpenAI connection\n",
    "* TPM: 50K (or max available)\n",
    "* Content filter: DefaultV2\n",
    "\n",
    "Wait for deployment to complete.\n",
    "\n",
    "---\n",
    "\n",
    "### Deploy gpt-4o-mini\n",
    "\n",
    "Repeat the same steps to deploy **gpt-4o-mini** with identical settings.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Manually Evaluate a Model\n",
    "\n",
    "Manual evaluation allows you to **inspect and score outputs by hand**.\n",
    "\n",
    "### Download Test Dataset\n",
    "\n",
    "1. Download:\n",
    "\n",
    "   * [https://raw.githubusercontent.com/MicrosoftLearning/mslearn-ai-studio/refs/heads/main/data/travel_evaluation_data.jsonl](https://raw.githubusercontent.com/MicrosoftLearning/mslearn-ai-studio/refs/heads/main/data/travel_evaluation_data.jsonl)\n",
    "2. Ensure the file is saved as **.jsonl** (not `.txt`)\n",
    "\n",
    "---\n",
    "\n",
    "### Create a Manual Evaluation\n",
    "\n",
    "1. In Foundry navigation, select **Protect and govern → Evaluation**\n",
    "2. Close the auto-open pane if needed\n",
    "3. Open **Manual evaluations** tab\n",
    "4. Select **+ New manual evaluation**\n",
    "\n",
    "---\n",
    "\n",
    "### Configure the Evaluation\n",
    "\n",
    "* **Model:** gpt-4o deployment\n",
    "* **System message:**\n",
    "\n",
    "```\n",
    "Assist users with travel-related inquiries, offering tips, advice, and recommendations as a knowledgeable travel agent.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Import Test Data\n",
    "\n",
    "1. Select **Import test data**\n",
    "2. Upload `travel_evaluation_data.jsonl`\n",
    "3. Map fields:\n",
    "\n",
    "   * **Input:** Question\n",
    "   * **Expected response:** ExpectedResponse\n",
    "\n",
    "Review the questions and expected answers.\n",
    "\n",
    "---\n",
    "\n",
    "### Run and Score\n",
    "\n",
    "1. Select **Run**\n",
    "2. Wait for outputs to be generated\n",
    "3. Compare model output vs expected response\n",
    "4. Score each response using **thumbs up / thumbs down**\n",
    "\n",
    "After scoring:\n",
    "\n",
    "* Review summary tiles\n",
    "* Select **Save results** and name the evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Automated Evaluation\n",
    "\n",
    "Automated evaluation provides **scalable, standardized metrics** and uses AI to assess quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Create an Automated Evaluation\n",
    "\n",
    "1. Return to **Evaluation** page\n",
    "2. Open **Automated evaluations** tab\n",
    "3. Select **Create a new evaluation**\n",
    "4. Choose **Evaluate a model**\n",
    "\n",
    "---\n",
    "\n",
    "### Select Data Source\n",
    "\n",
    "* Choose **Use your dataset**\n",
    "* Select the uploaded **travel_evaluation_data.jsonl** dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Configure Model Under Test\n",
    "\n",
    "* **Model:** gpt-4o-mini\n",
    "* **System message:** Same travel assistant prompt\n",
    "* **Query field:** `{{item.question}}`\n",
    "\n",
    "---\n",
    "\n",
    "### Configure Evaluators\n",
    "\n",
    "Add the following evaluators:\n",
    "\n",
    "#### 1. Model Scorer\n",
    "\n",
    "* Criteria: **Semantic_similarity**\n",
    "* Grade with: **gpt-4o**\n",
    "* Output: `{{sample.output_text}}`\n",
    "* Ground truth: `{{item.ExpectedResponse}}`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Likert-Scale Evaluator\n",
    "\n",
    "* Criteria: **Relevance**\n",
    "* Grade with: **gpt-4o**\n",
    "* Query: `{{item.question}}`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Text Similarity\n",
    "\n",
    "* Criteria: **F1_Score**\n",
    "* Ground truth: `{{item.ExpectedResponse}}`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Hateful and Unfair Content\n",
    "\n",
    "* Criteria: **Hate_and_unfairness**\n",
    "* Query: `{{item.question}}`\n",
    "\n",
    "---\n",
    "\n",
    "### Submit Evaluation\n",
    "\n",
    "1. Review configuration\n",
    "2. Give the evaluation a descriptive name\n",
    "3. Select **Submit**\n",
    "\n",
    "Wait for evaluation to complete. Use **Refresh** if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Review Evaluation Results\n",
    "\n",
    "### Metrics View\n",
    "\n",
    "* Review aggregated scores for each evaluator\n",
    "* Compare model performance quantitatively\n",
    "\n",
    "### Data View\n",
    "\n",
    "* Open the **Data** tab\n",
    "* Inspect per-question metrics\n",
    "* Review **AI-generated explanations** for scores\n",
    "\n",
    "**AI-102 insight:** Automated evaluation uses a **model-as-a-judge** pattern.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Clean Up\n",
    "\n",
    "To avoid Azure charges:\n",
    "\n",
    "1. Open **[https://portal.azure.com](https://portal.azure.com)**\n",
    "2. Go to **Resource groups**\n",
    "3. Select the group created for this lab\n",
    "4. Choose **Delete resource group**\n",
    "5. Confirm deletion\n",
    "\n",
    "---\n",
    "\n",
    "## AI-102 Exam Focus\n",
    "\n",
    "* Manual vs automated evaluation\n",
    "* Purpose of expected responses (ground truth)\n",
    "* Role of gpt-4o as an evaluator\n",
    "* Common metrics: semantic similarity, relevance, F1 score\n",
    "* Safety evaluation (hate and unfairness)\n",
    "* Why automated evaluation scales better"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
